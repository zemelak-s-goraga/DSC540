{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0141de2",
   "metadata": {},
   "source": [
    "# DSC540-T301_2245_1 Data Preparation\n",
    "\n",
    "Assignment Week 7 & 8 Excercise; \n",
    "\n",
    "Author: Zemelak Goraga; \n",
    "\n",
    "Date: 5/4/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ca26d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python codes for data cleaning and transformation of the '2017 Candy Data Raw.csv' dataset. \n",
    "\n",
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd8e55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with ISO-8859-1 encoding\n",
    "df = pd.read_csv('2017 Candy Data Raw.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f48d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current column names: ['Internal ID', 'Q1: GOING OUT?', 'Q2: GENDER', 'Q3: AGE', 'Q4: COUNTRY', 'Q5: STATE, PROVINCE, COUNTY, ETC', 'Q6 | 100 Grand Bar', 'Q6 | Anonymous brown globs that come in black and orange wrappers\\t(a.k.a. Mary Janes)', 'Q6 | Any full-sized candy bar', 'Q6 | Black Jacks', 'Q6 | Bonkers (the candy)', 'Q6 | Bonkers (the board game)', 'Q6 | Bottle Caps', \"Q6 | Box'o'Raisins\", 'Q6 | Broken glow stick', 'Q6 | Butterfinger', 'Q6 | Cadbury Creme Eggs', 'Q6 | Candy Corn', 'Q6 | Candy that is clearly just the stuff given out for free at restaurants', 'Q6 | Caramellos', 'Q6 | Cash, or other forms of legal tender', 'Q6 | Chardonnay', 'Q6 | Chick-o-Sticks (we donÕt know what that is)', 'Q6 | Chiclets', 'Q6 | Coffee Crisp', 'Q6 | Creepy Religious comics/Chick Tracts', 'Q6 | Dental paraphenalia', 'Q6 | Dots', 'Q6 | Dove Bars', 'Q6 | Fuzzy Peaches', 'Q6 | Generic Brand Acetaminophen', 'Q6 | Glow sticks', 'Q6 | Goo Goo Clusters', \"Q6 | Good N' Plenty\", 'Q6 | Gum from baseball cards', 'Q6 | Gummy Bears straight up', 'Q6 | Hard Candy', 'Q6 | Healthy Fruit', 'Q6 | Heath Bar', \"Q6 | Hershey's Dark Chocolate\", 'Q6 | HersheyÕs Milk Chocolate', \"Q6 | Hershey's Kisses\", 'Q6 | Hugs (actual physical hugs)', 'Q6 | Jolly Rancher (bad flavor)', 'Q6 | Jolly Ranchers (good flavor)', 'Q6 | JoyJoy (Mit Iodine!)', 'Q6 | Junior Mints', 'Q6 | Senior Mints', 'Q6 | Kale smoothie', 'Q6 | Kinder Happy Hippo', 'Q6 | Kit Kat', 'Q6 | LaffyTaffy', 'Q6 | LemonHeads', 'Q6 | Licorice (not black)', 'Q6 | Licorice (yes black)', 'Q6 | Lindt Truffle', 'Q6 | Lollipops', 'Q6 | Mars', 'Q6 | Maynards', 'Q6 | Mike and Ike', 'Q6 | Milk Duds', 'Q6 | Milky Way', 'Q6 | Regular M&Ms', 'Q6 | Peanut M&MÕs', \"Q6 | Blue M&M's\", \"Q6 | Red M&M's\", \"Q6 | Green Party M&M's\", \"Q6 | Independent M&M's\", \"Q6 | Abstained from M&M'ing.\", 'Q6 | Minibags of chips', 'Q6 | Mint Kisses', 'Q6 | Mint Juleps', 'Q6 | Mr. Goodbar', 'Q6 | Necco Wafers', 'Q6 | Nerds', 'Q6 | Nestle Crunch', \"Q6 | Now'n'Laters\", 'Q6 | Peeps', 'Q6 | Pencils', 'Q6 | Pixy Stix', 'Q6 | Real Housewives of Orange County Season 9 Blue-Ray', 'Q6 | ReeseÕs Peanut Butter Cups', \"Q6 | Reese's Pieces\", 'Q6 | Reggie Jackson Bar', 'Q6 | Rolos', 'Q6 | Sandwich-sized bags filled with BooBerry Crunch', 'Q6 | Skittles', 'Q6 | Smarties (American)', 'Q6 | Smarties (Commonwealth)', 'Q6 | Snickers', 'Q6 | Sourpatch Kids (i.e. abominations of nature)', 'Q6 | Spotted Dick', 'Q6 | Starburst', 'Q6 | Sweet Tarts', 'Q6 | Swedish Fish', 'Q6 | Sweetums (a friend to diabetes)', 'Q6 | Take 5', 'Q6 | Tic Tacs', 'Q6 | Those odd marshmallow circus peanut things', 'Q6 | Three Musketeers', 'Q6 | Tolberone something or other', 'Q6 | Trail Mix', 'Q6 | Twix', 'Q6 | Vials of pure high fructose corn syrup, for main-lining into your vein', 'Q6 | Vicodin', 'Q6 | Whatchamacallit Bars', 'Q6 | White Bread', 'Q6 | Whole Wheat anything', 'Q6 | York Peppermint Patties', 'Q7: JOY OTHER', 'Q8: DESPAIR OTHER', 'Q9: OTHER COMMENTS', 'Q10: DRESS', 'Unnamed: 113', 'Q11: DAY', 'Q12: MEDIA [Daily Dish]', 'Q12: MEDIA [Science]', 'Q12: MEDIA [ESPN]', 'Q12: MEDIA [Yahoo]', 'Click Coordinates (x, y)']\n"
     ]
    }
   ],
   "source": [
    "# Print current column names to check\n",
    "print(\"Current column names:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f53db540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated column names: ['Internal_ID', 'Going_Out', 'Gender', 'Age', 'Country', 'Region', 'Candy_100_Grand_Bar', 'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes', 'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks', 'Candy_Bonkers_the_candy', 'Candy_Bonkers_the_board_game', 'Candy_Bottle_Caps', 'Candy_BoxoRaisins', 'Candy_Broken_glow_stick', 'Candy_Butterfinger', 'Candy_Cadbury_Creme_Eggs', 'Candy_Candy_Corn', 'Candy_Candy_that_is_clearly_just_the_stuff_given_out_for_free_at_restaurants', 'Candy_Caramellos', 'Candy_Cash_or_other_forms_of_legal_tender', 'Candy_Chardonnay', 'Candy_Chick_o_Sticks_we_donÕt_know_what_that_is', 'Candy_Chiclets', 'Candy_Coffee_Crisp', 'Candy_Creepy_Religious_comics/Chick_Tracts', 'Candy_Dental_paraphenalia', 'Candy_Dots', 'Candy_Dove_Bars', 'Candy_Fuzzy_Peaches', 'Candy_Generic_Brand_Acetaminophen', 'Candy_Glow_sticks', 'Candy_Goo_Goo_Clusters', 'Candy_Good_N_Plenty', 'Candy_Gum_from_baseball_cards', 'Candy_Gummy_Bears_straight_up', 'Candy_Hard_Candy', 'Candy_Healthy_Fruit', 'Candy_Heath_Bar', 'Candy_Hersheys_Dark_Chocolate', 'Candy_HersheyÕs_Milk_Chocolate', 'Candy_Hersheys_Kisses', 'Candy_Hugs_actual_physical_hugs', 'Candy_Jolly_Rancher_bad_flavor', 'Candy_Jolly_Ranchers_good_flavor', 'Candy_JoyJoy_Mit_Iodine!', 'Candy_Junior_Mints', 'Candy_Senior_Mints', 'Candy_Kale_smoothie', 'Candy_Kinder_Happy_Hippo', 'Candy_Kit_Kat', 'Candy_LaffyTaffy', 'Candy_LemonHeads', 'Candy_Licorice_not_black', 'Candy_Licorice_yes_black', 'Candy_Lindt_Truffle', 'Candy_Lollipops', 'Candy_Mars', 'Candy_Maynards', 'Candy_Mike_and_Ike', 'Candy_Milk_Duds', 'Candy_Milky_Way', 'Candy_Regular_MandMs', 'Candy_Peanut_MandMÕs', 'Candy_Blue_MandMs', 'Candy_Red_MandMs', 'Candy_Green_Party_MandMs', 'Candy_Independent_MandMs', 'Candy_Abstained_from_MandMing.', 'Candy_Minibags_of_chips', 'Candy_Mint_Kisses', 'Candy_Mint_Juleps', 'Candy_Mr._Goodbar', 'Candy_Necco_Wafers', 'Candy_Nerds', 'Candy_Nestle_Crunch', 'Candy_NownLaters', 'Candy_Peeps', 'Candy_Pencils', 'Candy_Pixy_Stix', 'Candy_Real_Housewives_of_Orange_County_Season_9_Blue_Ray', 'Candy_ReeseÕs_Peanut_Butter_Cups', 'Candy_Reeses_Pieces', 'Candy_Reggie_Jackson_Bar', 'Candy_Rolos', 'Candy_Sandwich_sized_bags_filled_with_BooBerry_Crunch', 'Candy_Skittles', 'Candy_Smarties_American', 'Candy_Smarties_Commonwealth', 'Candy_Snickers', 'Candy_Sourpatch_Kids_i.e._abominations_of_nature', 'Candy_Spotted_Dick', 'Candy_Starburst', 'Candy_Sweet_Tarts', 'Candy_Swedish_Fish', 'Candy_Sweetums_a_friend_to_diabetes', 'Candy_Take_5', 'Candy_Tic_Tacs', 'Candy_Those_odd_marshmallow_circus_peanut_things', 'Candy_Three_Musketeers', 'Candy_Tolberone_something_or_other', 'Candy_Trail_Mix', 'Candy_Twix', 'Candy_Vials_of_pure_high_fructose_corn_syrup_for_main_lining_into_your_vein', 'Candy_Vicodin', 'Candy_Whatchamacallit_Bars', 'Candy_White_Bread', 'Candy_Whole_Wheat_anything', 'Candy_York_Peppermint_Patties', 'Joy_Other', 'Despair_Other', 'Other_Comments', 'Dress_Code', 'Unnamed_Column', 'Survey_Day', 'Media_Daily_Dish', 'Media_Science', 'Media_ESPN', 'Media_Yahoo', 'Click_Coordinates']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('2017 Candy Data Raw.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define a dictionary to map the old column names to new, simpler names\n",
    "column_renames = {\n",
    "    'Internal ID': 'Internal_ID',\n",
    "    'Q1: GOING OUT?': 'Going_Out',\n",
    "    'Q2: GENDER': 'Gender',\n",
    "    'Q3: AGE': 'Age',\n",
    "    'Q4: COUNTRY': 'Country',\n",
    "    'Q5: STATE, PROVINCE, COUNTY, ETC': 'Region',\n",
    "    'Q7: JOY OTHER': 'Joy_Other',\n",
    "    'Q8: DESPAIR OTHER': 'Despair_Other',\n",
    "    'Q9: OTHER COMMENTS': 'Other_Comments',\n",
    "    'Q10: DRESS': 'Dress_Code',\n",
    "    'Q11: DAY': 'Survey_Day',\n",
    "    'Q12: MEDIA [Daily Dish]': 'Media_Daily_Dish',\n",
    "    'Q12: MEDIA [Science]': 'Media_Science',\n",
    "    'Q12: MEDIA [ESPN]': 'Media_ESPN',\n",
    "    'Q12: MEDIA [Yahoo]': 'Media_Yahoo',\n",
    "    'Click Coordinates (x, y)': 'Click_Coordinates',\n",
    "    'Unnamed: 113': 'Unnamed_Column'\n",
    "}\n",
    "\n",
    "# Automatically generate new names for all candy columns by replacing and simplifying text\n",
    "for col in df.columns:\n",
    "    if 'Q6 |' in col:\n",
    "        new_col_name = col.replace('Q6 | ', '').replace('\\t', ' ').replace('  ', ' ').replace(',', '').replace(\"'\", '').replace('(', '').replace(')', '')\n",
    "        new_col_name = 'Candy_' + '_'.join(new_col_name.split()).replace('-', '_').replace('&', 'and')\n",
    "        column_renames[col] = new_col_name\n",
    "\n",
    "# Apply the renaming\n",
    "df.rename(columns=column_renames, inplace=True)\n",
    "\n",
    "# Print the updated column names to verify\n",
    "print(\"Updated column names:\", df.columns.tolist())\n",
    "\n",
    "# Optionally, save the renamed DataFrame to a new CSV for further use\n",
    "df.to_csv('Updated_Candy_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a7372",
   "metadata": {},
   "source": [
    "# Chapter 7: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98ca219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 7: Data Cleaning\n",
    "# Filter out missing data\n",
    "df.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d7ac047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing data\n",
    "df['Gender'].fillna('Unknown', inplace=True)\n",
    "df['Region'].fillna('USA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80140432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(subset='Internal_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbe41d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values\n",
    "country_replacements = {\n",
    "    'USA': 'United States', 'us': 'United States', 'U.S.A.': 'United States', 'U.S.': 'United States',\n",
    "    'CAN': 'Canada', 'ca': 'Canada', 'C.A.': 'Canada'\n",
    "}\n",
    "df['Country'] = df['Country'].replace(country_replacements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b2fd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "df['Going_Out'] = df['Going_Out'].map({'Yes': True, 'No': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85e7330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate strings using the new column name\n",
    "df['Region'] = df['Region'].str.title()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1730c",
   "metadata": {},
   "source": [
    "# Chapter 8: Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84574882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 8: Data Transformation\n",
    "# Create hierarchical index using the updated column names\n",
    "df.set_index(['Internal_ID', 'Country'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2a9276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Going_Out', 'Gender', 'Age', 'Region', 'Candy_100_Grand_Bar',\n",
      "       'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes',\n",
      "       'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks',\n",
      "       'Candy_Bonkers_the_candy', 'Candy_Bonkers_the_board_game',\n",
      "       ...\n",
      "       'Despair_Other', 'Other_Comments', 'Dress_Code', 'Unnamed_Column',\n",
      "       'Survey_Day', 'Media_Daily_Dish', 'Media_Science', 'Media_ESPN',\n",
      "       'Media_Yahoo', 'Click_Coordinates'],\n",
      "      dtype='object', length=118)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61b4d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hierarchical index with the renamed columns\n",
    "#df.set_index(['Age', 'Region'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "564f1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape using pivot with the updated column names\n",
    "pivot_df = df.pivot_table(index='Gender', columns='Candy_100_Grand_Bar', values='Age', aggfunc='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "225f6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current candy column names:\n",
      "['Candy_100_Grand_Bar', 'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes', 'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks', 'Candy_Bonkers_the_candy', 'Candy_Bonkers_the_board_game', 'Candy_Bottle_Caps', 'Candy_BoxoRaisins', 'Candy_Broken_glow_stick', 'Candy_Butterfinger', 'Candy_Cadbury_Creme_Eggs', 'Candy_Candy_Corn', 'Candy_Candy_that_is_clearly_just_the_stuff_given_out_for_free_at_restaurants', 'Candy_Caramellos', 'Candy_Cash_or_other_forms_of_legal_tender', 'Candy_Chardonnay', 'Candy_Chick_o_Sticks_we_donÕt_know_what_that_is', 'Candy_Chiclets', 'Candy_Coffee_Crisp', 'Candy_Creepy_Religious_comics/Chick_Tracts', 'Candy_Dental_paraphenalia', 'Candy_Dots', 'Candy_Dove_Bars', 'Candy_Fuzzy_Peaches', 'Candy_Generic_Brand_Acetaminophen', 'Candy_Glow_sticks', 'Candy_Goo_Goo_Clusters', 'Candy_Good_N_Plenty', 'Candy_Gum_from_baseball_cards', 'Candy_Gummy_Bears_straight_up', 'Candy_Hard_Candy', 'Candy_Healthy_Fruit', 'Candy_Heath_Bar', 'Candy_Hersheys_Dark_Chocolate', 'Candy_HersheyÕs_Milk_Chocolate', 'Candy_Hersheys_Kisses', 'Candy_Hugs_actual_physical_hugs', 'Candy_Jolly_Rancher_bad_flavor', 'Candy_Jolly_Ranchers_good_flavor', 'Candy_JoyJoy_Mit_Iodine!', 'Candy_Junior_Mints', 'Candy_Senior_Mints', 'Candy_Kale_smoothie', 'Candy_Kinder_Happy_Hippo', 'Candy_Kit_Kat', 'Candy_LaffyTaffy', 'Candy_LemonHeads', 'Candy_Licorice_not_black', 'Candy_Licorice_yes_black', 'Candy_Lindt_Truffle', 'Candy_Lollipops', 'Candy_Mars', 'Candy_Maynards', 'Candy_Mike_and_Ike', 'Candy_Milk_Duds', 'Candy_Milky_Way', 'Candy_Regular_MandMs', 'Candy_Peanut_MandMÕs', 'Candy_Blue_MandMs', 'Candy_Red_MandMs', 'Candy_Green_Party_MandMs', 'Candy_Independent_MandMs', 'Candy_Abstained_from_MandMing.', 'Candy_Minibags_of_chips', 'Candy_Mint_Kisses', 'Candy_Mint_Juleps', 'Candy_Mr._Goodbar', 'Candy_Necco_Wafers', 'Candy_Nerds', 'Candy_Nestle_Crunch', 'Candy_NownLaters', 'Candy_Peeps', 'Candy_Pencils', 'Candy_Pixy_Stix', 'Candy_Real_Housewives_of_Orange_County_Season_9_Blue_Ray', 'Candy_ReeseÕs_Peanut_Butter_Cups', 'Candy_Reeses_Pieces', 'Candy_Reggie_Jackson_Bar', 'Candy_Rolos', 'Candy_Sandwich_sized_bags_filled_with_BooBerry_Crunch', 'Candy_Skittles', 'Candy_Smarties_American', 'Candy_Smarties_Commonwealth', 'Candy_Snickers', 'Candy_Sourpatch_Kids_i.e._abominations_of_nature', 'Candy_Spotted_Dick', 'Candy_Starburst', 'Candy_Sweet_Tarts', 'Candy_Swedish_Fish', 'Candy_Sweetums_a_friend_to_diabetes', 'Candy_Take_5', 'Candy_Tic_Tacs', 'Candy_Those_odd_marshmallow_circus_peanut_things', 'Candy_Three_Musketeers', 'Candy_Tolberone_something_or_other', 'Candy_Trail_Mix', 'Candy_Twix', 'Candy_Vials_of_pure_high_fructose_corn_syrup_for_main_lining_into_your_vein', 'Candy_Vicodin', 'Candy_Whatchamacallit_Bars', 'Candy_White_Bread', 'Candy_Whole_Wheat_anything', 'Candy_York_Peppermint_Patties']\n",
      "Preference                                          DESPAIR   JOY  MEH\n",
      "Candy                                                                 \n",
      "Candy_100_Grand_Bar                                      85   873  755\n",
      "Candy_Abstained_from_MandMing.                          693   218  607\n",
      "Candy_Anonymous_brown_globs_that_come_in_black_...     1089   176  461\n",
      "Candy_Any_full_sized_candy_bar                           17  1559  212\n",
      "Candy_Black_Jacks                                       793    92  617\n",
      "...                                                     ...   ...  ...\n",
      "Candy_Vicodin                                           723   707  241\n",
      "Candy_Whatchamacallit_Bars                              288   840  509\n",
      "Candy_White_Bread                                      1455    44  204\n",
      "Candy_Whole_Wheat_anything                             1289   117  307\n",
      "Candy_York_Peppermint_Patties                           232  1105  418\n",
      "\n",
      "[103 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Pivot the data to summarize candy preferences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assume df is your DataFrame and columns have been correctly renamed\n",
    "# For example:\n",
    "# df.rename(columns={old_name: new_name for old_name, new_name in ...}, inplace=True)\n",
    "\n",
    "# Verify current candy column names\n",
    "print(\"Current candy column names:\")\n",
    "print([col for col in df.columns if col.startswith('Candy_')])\n",
    "\n",
    "# Filter to include only candy columns and melt for reshaping\n",
    "candy_responses = df.filter(like='Candy_').melt(var_name='Candy', value_name='Preference')\n",
    "\n",
    "# Create a pivot table to summarize candy preferences\n",
    "summary_pivot = candy_responses.pivot_table(index='Candy', columns='Preference', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Print the pivot table to verify it looks correct\n",
    "print(summary_pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7dc78",
   "metadata": {},
   "source": [
    "# Chapter 9: Advanced Data Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07404f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Age Age Group\n",
      "Internal_ID Country                      \n",
      "90258773    NaN             NaN       NaN\n",
      "90272821    USA            44.0     35-49\n",
      "90272829    United States  49.0     35-49\n",
      "90272840    United States  40.0     35-49\n",
      "90272841    usa            23.0     18-34\n"
     ]
    }
   ],
   "source": [
    "# Chapter 9: Advanced Data Grouping\n",
    "# Grouping with Dicts/Series\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame\n",
    "# df = pd.read_csv('path_to_your_data.csv')\n",
    "\n",
    "# Check and handle non-numeric values in the 'Age' column\n",
    "# Convert all non-numeric entries to NaN or handle them appropriately\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # 'coerce' will convert invalid parsing to NaN\n",
    "\n",
    "# Optional: Fill NaN values if necessary, e.g., with the mean or median of the column\n",
    "# df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "\n",
    "# Define age bins and labels\n",
    "age_bins = [0, 18, 35, 50, 65, 100]\n",
    "labels = ['Under 18', '18-34', '35-49', '50-64', '65+']\n",
    "\n",
    "# Create a new column for age groups using pd.cut\n",
    "df['Age Group'] = pd.cut(df['Age'], bins=age_bins, labels=labels)\n",
    "\n",
    "# Print some of the DataFrame to verify the changes\n",
    "print(df[['Age', 'Age Group']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cee18a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping with Functions\n",
    "# Assuming 'Country' is the renamed column for 'Q4: COUNTRY'\n",
    "# Check if the DataFrame has been reset and indexed properly to avoid errors when resetting\n",
    "if 'Country' not in df.columns:\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "df['Country First Letter'] = df['Country'].apply(lambda x: x[0] if pd.notnull(x) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a661af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n",
      "'merica                            JOY\n",
      "1                                   []\n",
      "32                                  []\n",
      "35                                  []\n",
      "45                                  []\n",
      "                               ...    \n",
      "united states                      JOY\n",
      "united states of america    [JOY, MEH]\n",
      "united ststes                       []\n",
      "usa                                JOY\n",
      "usas                                []\n",
      "Name: Candy_100_Grand_Bar, Length: 124, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Split/Apply/Combine\n",
    "# Group by 'Country' which was previously 'Q4: COUNTRY'\n",
    "grouped = df.groupby('Country')\n",
    "\n",
    "# Assuming 'Q6 | 100 Grand Bar' was renamed to 'Candy_100_Grand_Bar'\n",
    "# Calculate the mode of preferences for the 'Candy_100_Grand_Bar'\n",
    "mode_of_preferences = grouped['Candy_100_Grand_Bar'].agg(pd.Series.mode)\n",
    "\n",
    "# Print the results to verify\n",
    "print(mode_of_preferences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7a957a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Internal_ID', 'Country', 'Going_Out', 'Gender', 'Age', 'Region',\n",
      "       'Candy_100_Grand_Bar',\n",
      "       'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes',\n",
      "       'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks',\n",
      "       ...\n",
      "       'Dress_Code', 'Unnamed_Column', 'Survey_Day', 'Media_Daily_Dish',\n",
      "       'Media_Science', 'Media_ESPN', 'Media_Yahoo', 'Click_Coordinates',\n",
      "       'Age Group', 'Country First Letter'],\n",
      "      dtype='object', length=122)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386060fa",
   "metadata": {},
   "source": [
    "# Chapter 10: Further Data Grouping and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6535c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Internal_ID', 'Country', 'Going_Out', 'Gender', 'Age', 'Region',\n",
      "       'Candy_100_Grand_Bar',\n",
      "       'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes',\n",
      "       'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks',\n",
      "       ...\n",
      "       'Dress_Code', 'Unnamed_Column', 'Survey_Day', 'Media_Daily_Dish',\n",
      "       'Media_Science', 'Media_ESPN', 'Media_Yahoo', 'Click_Coordinates',\n",
      "       'Age Group', 'Country First Letter'],\n",
      "      dtype='object', length=122)\n",
      "Country                   Candy_100_Grand_Bar\n",
      "'merica                   JOY                     1\n",
      "A                         MEH                     1\n",
      "Ahem....Amerca            MEH                     1\n",
      "Alaska                    JOY                     1\n",
      "America                   JOY                     1\n",
      "                                                 ..\n",
      "united states of america  JOY                     2\n",
      "                          MEH                     2\n",
      "usa                       JOY                    94\n",
      "                          MEH                    57\n",
      "                          DESPAIR                 7\n",
      "Name: Candy_100_Grand_Bar, Length: 130, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Chapter 10: Further Data Grouping and Analysis\n",
    "# Grouping with Index Levels\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame (assumed to be already loaded and columns correctly named as shown)\n",
    "# df = pd.read_csv('path_to_your_data.csv')\n",
    "\n",
    "# Print current column names to confirm setup\n",
    "print(df.columns)\n",
    "\n",
    "# Group by the 'Country' column since it's not an index level\n",
    "grouped = df.groupby('Country')\n",
    "\n",
    "# Count the values for 'Candy_100_Grand_Bar' within each country\n",
    "country_group_summary = grouped['Candy_100_Grand_Bar'].value_counts()\n",
    "\n",
    "# Print the results to verify\n",
    "print(country_group_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23805bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Internal_ID', 'Country', 'Going_Out', 'Gender', 'Age', 'Region',\n",
      "       'Candy_100_Grand_Bar',\n",
      "       'Candy_Anonymous_brown_globs_that_come_in_black_and_orange_wrappers_a.k.a._Mary_Janes',\n",
      "       'Candy_Any_full_sized_candy_bar', 'Candy_Black_Jacks',\n",
      "       ...\n",
      "       'Dress_Code', 'Unnamed_Column', 'Survey_Day', 'Media_Daily_Dish',\n",
      "       'Media_Science', 'Media_ESPN', 'Media_Yahoo', 'Click_Coordinates',\n",
      "       'Age Group', 'Country First Letter'],\n",
      "      dtype='object', length=122)\n",
      "Candy_100_Grand_Bar  DESPAIR  JOY  MEH\n",
      "Gender                                \n",
      "Female                    39  265  279\n",
      "I'd rather not say         3   33   24\n",
      "Male                      41  564  436\n",
      "Other                      2    8   12\n",
      "Unknown                    0    3    4\n"
     ]
    }
   ],
   "source": [
    "# Cross Tabs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is already loaded and columns are named as 'Gender' and 'Candy_100_Grand_Bar'\n",
    "# Here's how to confirm the column names\n",
    "print(df.columns)\n",
    "\n",
    "# If 'Gender' or 'Candy_100_Grand_Bar' are part of the index and you need to access them:\n",
    "if 'Gender' not in df.columns or 'Candy_100_Grand_Bar' not in df.columns:\n",
    "    df.reset_index(inplace=True)  # Reset index if necessary\n",
    "\n",
    "# Create a cross-tabulation of the number of occurrences for each combination\n",
    "cross_tab = pd.crosstab(df['Gender'], df['Candy_100_Grand_Bar'])\n",
    "\n",
    "# Print the resulting cross-tabulation to check the results\n",
    "print(cross_tab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35aa6c9",
   "metadata": {},
   "source": [
    "# Additional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d8cf3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Methods\n",
    "# Data Type Conversions\n",
    "df['Gender'] = df['Gender'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acae04b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2348.000000\n",
      "mean       42.057666\n",
      "std        12.084156\n",
      "min         1.000000\n",
      "25%        34.000000\n",
      "50%        41.000000\n",
      "75%        50.000000\n",
      "max       100.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Error Detection and Correction\n",
    "# Here we could check for illogical age values as an example\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'Age' has been loaded or processed before\n",
    "# Convert 'Age' to a numeric column, coercing errors to NaN\n",
    "df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "\n",
    "# Now, safely filter and replace values\n",
    "df.loc[df['Age'] > 100, 'Age'] = np.nan\n",
    "\n",
    "# Optional: Check results\n",
    "print(df['Age'].describe())  # To see the summary statistics including max, mean, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5880b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv('2017CandyDataCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e3f4259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference                                          DESPAIR   JOY  MEH\n",
      "Candy                                                                 \n",
      "Candy_100_Grand_Bar                                      85   873  755\n",
      "Candy_Abstained_from_MandMing.                          693   218  607\n",
      "Candy_Anonymous_brown_globs_that_come_in_black_...     1089   176  461\n",
      "Candy_Any_full_sized_candy_bar                           17  1559  212\n",
      "Candy_Black_Jacks                                       793    92  617\n"
     ]
    }
   ],
   "source": [
    "# Print summary pivot for verification\n",
    "print(summary_pivot.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d8945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f78011d",
   "metadata": {},
   "source": [
    "# Project Report: \n",
    "\n",
    "Topic: Analysis of the 2017 So Much Data Candy Survey: Data cleaning and transformation\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "This report offers a detailed exploration of the 2017 So Much Data Candy Survey, emphasizing the meticulous cleaning and transformation of the survey data to enable more robust statistical analysis and richer data interpretation. By addressing various data inconsistencies, missing values, and anomalies commonly encountered in survey datasets, the primary objective was to refine the data for advanced analytics. Through strategic cleaning and transformation techniques, the data is now poised for insightful analysis, providing a reliable foundation for understanding candy preferences during Halloween.\n",
    "\n",
    "Introduction:\n",
    "\n",
    "The 2017 So Much Data Candy Survey was designed to capture public preferences concerning various types of candy during the Halloween season. The raw data, however, like many similar datasets, suffered from several issues that could potentially distort analytical outcomes if not addressed. These issues ranged from incomplete data entries to inconsistently formatted responses, necessitating a rigorous systematic approach to data cleaning and transformation to ensure data quality and reliability for subsequent analyses.\n",
    "\n",
    "Statement of the Problem:\n",
    "\n",
    "The dataset initially presented multiple data integrity challenges, including missing values, duplicates, inconsistent entries, and poorly formatted responses. These issues are detrimental as they can lead to inaccurate data analyses and potentially misleading conclusions, affecting decision-making processes based on this data. Thus, a focused effort to rectify these problems was essential to maintain the integrity and enhance the usability of the dataset for reliable and valid statistical analysis.\n",
    "\n",
    "Methodology:\n",
    "\n",
    "The methodology for refining the dataset incorporated a variety of data cleaning and transformation techniques detailed across several chapters of established data processing literature:\n",
    "\n",
    "Chapter 7: Data Cleaning\n",
    "Filter out missing data: Rows completely empty were removed to ensure data quality.\n",
    "Fill in missing data: Critical demographic fields such as 'GENDER' and 'COUNTRY' were filled with 'Unknown' and 'USA' respectively to eliminate gaps in the data.\n",
    "Remove duplicates: Entries with duplicate 'Internal ID' were eliminated to ensure the uniqueness and integrity of each data record.\n",
    "Replace values: Standardized entries for country and gender to a common format to maintain consistency across the dataset.\n",
    "Transform data: Responses to the categorical question \"GOING OUT?\" were converted to boolean values (True/False) to simplify subsequent analyses.\n",
    "Manipulate strings: State names were formatted to title case to ensure consistency in textual data representation.\n",
    "\n",
    "Chapter 8: Data Transformation\n",
    "Create hierarchical index: A new hierarchical index using 'Internal ID' and 'COUNTRY' was created to facilitate complex data queries and aggregation.\n",
    "Reshape: The dataset was reshaped using pivot tables, a technique that helps summarize and organize data effectively, focusing particularly on candy preferences.\n",
    "Pivot the data: The 'CANDY' columns were pivoted to better summarize and analyze responses, allowing for a clear depiction of preferences across various types of candy.\n",
    "\n",
    "Chapter 9: Advanced Data Grouping\n",
    "Grouping with Dicts/Series: Age data were grouped into bins using a mapping of age ranges to predefined labels, aiding in demographic segmentation.\n",
    "Grouping with Functions: Custom functions were utilized to group data based on the first letter of the country, enabling geographic-based analyses.\n",
    "Split/Apply/Combine: Data was split by 'COUNTRY', operations were applied to compute the mode of candy preferences, and results were combined to give a comprehensive view of national preferences.\n",
    "\n",
    "Chapter 10: Further Data Grouping and Analysis\n",
    "Grouping with Index Levels: Employed multi-level indexes for generating grouped summaries, enhancing the granularity of insights that can be extracted.\n",
    "Cross Tabs: Cross-tabulations were created to analyze the relationships between gender and candy preferences, providing insights into demographic trends.\n",
    "Grouping with Functions: Functions were defined for custom grouping based on various demographic data points to explore deeper relational dynamics.\n",
    "\n",
    "Additional Methods\n",
    "Data Type Conversions: All data types were reviewed and adjusted as necessary, converting strings to categorical data types where it enhanced processing efficiency.\n",
    "Error Detection and Correction: The dataset was thoroughly scanned to identify and correct illogical or out-of-range values, ensuring the data's accuracy and reliability.\n",
    "\n",
    "These meticulous steps in data cleaning and transformation ensure that the dataset not only meets the quality standards required for advanced analytics but also remains versatile for a range of analytical applications. The refined data provides a robust foundation for generating reliable insights and supporting data-driven decision-making processes.\n",
    "\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Post-implementation of our data cleaning steps, the dataset exhibited no missing values in crucial columns such as 'Gender' and 'Country', ensuring a complete dataset for detailed analysis. The removal of duplicates guaranteed that each response was unique, accurately representing individual preferences. Moreover, the standardization of entries across categorical fields and the improved data formatting enhanced the dataset’s readability and usability, making it ready for rigorous analytical tasks.\n",
    "\n",
    "Discussion:\n",
    "\n",
    "The applied data cleaning and transformation processes have significantly elevated the quality of the dataset. These enhancements have laid a solid foundation for conducting reliable statistical analyses, including regression models, clustering, and frequency analysis of candy preferences. Notably, the transformation techniques such as data pivoting have systematically organized the data, enabling the effective observation of trends and patterns which are critical for deep analytical insights.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "The rigorous data cleaning and transformation efforts have rendered the 2017 Candy Data robust, clean, and highly suitable for detailed analytical scrutiny. It is anticipated that the processed dataset will yield more accurate insights into candy preferences, substantially aiding in data-driven decision-making and forecasting for future candy distribution strategies during Halloween.\n",
    "\n",
    "Way Forward:\n",
    "\n",
    "With the dataset now meticulously cleaned and transformed, it is ready for various sophisticated statistical analyses to unearth deeper insights. Potential future studies could explore candy preferences across different demographics, employ predictive modeling to foresee future trends in candy popularity, or conduct geographical analyses to examine regional variations in candy preferences. Integrating this dataset with additional datasets, such as retail sales data during the Halloween season, could further illuminate the impact of these preferences on actual sales, providing a more comprehensive view of market dynamics. Ongoing updates and meticulous cleaning of new survey data will be crucial to ensure the continued relevance and accuracy of the analysis, supporting sustained data-driven insights into consumer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb9552",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
